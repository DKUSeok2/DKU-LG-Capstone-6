{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.26.4\n",
      "Scikit-learn version: 1.4.2\n",
      "Pandas version: 2.2.2\n",
      "Matplotlib version: 3.8.4\n",
      "Seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# 각 라이브러리 버전 출력\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Matplotlib version:\", matplotlib.__version__)\n",
    "print(\"Seaborn version:\", sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import  OrdinalEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 원본 train 데이터 로드\u001b[39;00m\n\u001b[0;32m     16\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../shared codes/data/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../shared codes/data/test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# missing_value_removal_function 사용\u001b[39;00m\n\u001b[0;32m     20\u001b[0m train \u001b[38;5;241m=\u001b[39m missing_value_removal_function(train)\n",
      "File \u001b[1;32mc:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 현재 작업 디렉토리 경로를 가져와 shared codes 폴더의 위치를 sys.path에 추가합니다.\n",
    "# sys.path에 추가된 경로에 있는 py 폴더는 임포트할 수 있다.\n",
    "current_dir = os.getcwd()\n",
    "shared_codes_dir = os.path.join(current_dir, '../shared codes')\n",
    "sys.path.append(shared_codes_dir)\n",
    "\n",
    "\n",
    "# cover_nan 모듈을 임포트\n",
    "from cover_nan import missing_value_removal_function\n",
    "\n",
    "# 원본 train 데이터 로드\n",
    "train = pd.read_csv(\"../shared codes/data/train.csv\").drop(columns=['ID'])\n",
    "test = pd.read_csv(\"../shared codes/data/test.csv\").drop(columns=['ID'])\n",
    "\n",
    "# missing_value_removal_function 사용\n",
    "train = missing_value_removal_function(train)\n",
    "test = missing_value_removal_function(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('임신 성공 여부', axis=1)\n",
    "y = train['임신 성공 여부']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_fill_zero = [\n",
    "    # 'PGD 시술 여부', 'PGS 시술 여부',\n",
    "    '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부'\n",
    "]\n",
    "X[columns_fill_zero] = X[columns_fill_zero].fillna(0)\n",
    "test[columns_fill_zero] = test[columns_fill_zero].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['난자 채취 경과일'].fillna(1, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['난자 해동 경과일'].fillna(0, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['난자 혼합 경과일'].fillna(0, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['배아 이식 경과일'].fillna(0, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['배아 해동 경과일'].fillna(0, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['난자 채취 경과일'].fillna(1, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['난자 해동 경과일'].fillna(0, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['난자 혼합 경과일'].fillna(0, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['배아 이식 경과일'].fillna(0, inplace=True)\n",
      "C:\\Users\\hoyeong\\AppData\\Local\\Temp\\ipykernel_13184\\1085533716.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['배아 해동 경과일'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X['난자 채취 경과일'].fillna(1, inplace=True)\n",
    "X['난자 해동 경과일'].fillna(0, inplace=True)\n",
    "X['난자 혼합 경과일'].fillna(0, inplace=True)\n",
    "X['배아 이식 경과일'].fillna(0, inplace=True)\n",
    "X['배아 해동 경과일'].fillna(0, inplace=True)\n",
    "\n",
    "test['난자 채취 경과일'].fillna(1, inplace=True)\n",
    "test['난자 해동 경과일'].fillna(0, inplace=True)\n",
    "test['난자 혼합 경과일'].fillna(0, inplace=True)\n",
    "test['배아 이식 경과일'].fillna(0, inplace=True)\n",
    "test['배아 해동 경과일'].fillna(0, inplace=True)\n",
    "\n",
    "# 병합할 칼럼들\n",
    "columns_to_merge = [\n",
    "    '난자 채취 경과일', '난자 해동 경과일', '난자 혼합 경과일',\n",
    "    '배아 이식 경과일', '배아 해동 경과일'\n",
    "]\n",
    "\n",
    "# 새로운 칼럼 생성: 각 경과일의 합\n",
    "X['총 경과일'] = X[columns_to_merge].sum(axis=1)\n",
    "\n",
    "# 기존 칼럼 제거\n",
    "X = X.drop(columns=columns_to_merge)\n",
    "\n",
    "# 새로운 칼럼 생성: 각 경과일의 합\n",
    "test['총 경과일'] = test[columns_to_merge].sum(axis=1)\n",
    "\n",
    "# 기존 칼럼 제거\n",
    "test = test.drop(columns=columns_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측값이 있는 열과 개수:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# 각 열의 결측값 개수 확인\n",
    "missing_values_count = X.isnull().sum()\n",
    "\n",
    "# 결측값이 있는 열만 필터링\n",
    "missing_columns = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "print(\"결측값이 있는 열과 개수:\")\n",
    "print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ '알 수 없음' 값을 포함하는 컬럼들:\n",
      "['시술 당시 나이', '배란 유도 유형', '난자 기증자 나이', '정자 기증자 나이']\n"
     ]
    }
   ],
   "source": [
    "# '알 수 없음'이 포함된 열 찾기\n",
    "unknown_columns = []\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].astype(str).str.contains('알 수 없음').any():\n",
    "        unknown_columns.append(col)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"⚠️ '알 수 없음' 값을 포함하는 컬럼들:\")\n",
    "print(unknown_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 'nan' 값을 포함하는 컬럼들:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# '알 수 없음'이 포함된 열 찾기\n",
    "unknown_columns = []\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].astype(str).str.contains('nan').any():\n",
    "        unknown_columns.append(col)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"⚠️ 'nan' 값을 포함하는 컬럼들:\")\n",
    "print(unknown_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN 값이 존재하는 컬럼 찾기\n",
    "unknown_columns = X.columns[X.isna().any()].tolist()\n",
    "\n",
    "unknown_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Categorical(범주형) 칼럼 리스트:\n",
      "['시술 시기 코드', '시술 당시 나이', '시술 유형', '특정 시술 유형', '배란 유도 유형', '총 시술 횟수', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수', 'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '난자 출처', '정자 출처', '난자 기증자 나이', '정자 기증자 나이']\n"
     ]
    }
   ],
   "source": [
    "# Categorical(범주형) 칼럼 찾기\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"📌 Categorical(범주형) 칼럼 리스트:\")\n",
    "print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train_encoded = X.copy()\n",
    "X_train_encoded[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "X_test_encoded = test.copy()\n",
    "X_test_encoded[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "        \"남성 주 불임 원인\",\n",
    "        \"남성 부 불임 원인\",\n",
    "        \"불임 원인 - 정자 농도\",\n",
    "        \"불임 원인 - 정자 면역학적 요인\",\n",
    "        \"불임 원인 - 정자 운동성\",\n",
    "        \"불임 원인 - 정자 형태\",\n",
    "        '정자 기증자 나이',\n",
    "        '배란 유도 유형'\n",
    "]\n",
    "X_train_encoded = X_train_encoded.drop(columns = columns_to_drop)    \n",
    "X_test_encoded = X_test_encoded.drop(columns = columns_to_drop)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측값이 있는 열과 개수:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# 각 열의 결측값 개수 확인\n",
    "missing_values_count = X_train_encoded.isnull().sum()\n",
    "\n",
    "# 결측값이 있는 열만 필터링\n",
    "missing_columns = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "print(\"결측값이 있는 열과 개수:\")\n",
    "print(missing_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 256351 entries, 0 to 256350\n",
      "Data columns (total 52 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   시술 시기 코드          256351 non-null  float64\n",
      " 1   시술 당시 나이          256351 non-null  float64\n",
      " 2   시술 유형             256351 non-null  float64\n",
      " 3   특정 시술 유형          256351 non-null  float64\n",
      " 4   배란 자극 여부          256351 non-null  int64  \n",
      " 5   단일 배아 이식 여부       256351 non-null  float64\n",
      " 6   착상 전 유전 검사 사용 여부  256351 non-null  float64\n",
      " 7   착상 전 유전 진단 사용 여부  256351 non-null  float64\n",
      " 8   여성 주 불임 원인        256351 non-null  int64  \n",
      " 9   여성 부 불임 원인        256351 non-null  int64  \n",
      " 10  부부 주 불임 원인        256351 non-null  int64  \n",
      " 11  부부 부 불임 원인        256351 non-null  int64  \n",
      " 12  불명확 불임 원인         256351 non-null  int64  \n",
      " 13  불임 원인 - 난관 질환     256351 non-null  int64  \n",
      " 14  불임 원인 - 남성 요인     256351 non-null  int64  \n",
      " 15  불임 원인 - 배란 장애     256351 non-null  int64  \n",
      " 16  불임 원인 - 여성 요인     256351 non-null  int64  \n",
      " 17  불임 원인 - 자궁경부 문제   256351 non-null  int64  \n",
      " 18  불임 원인 - 자궁내막증     256351 non-null  int64  \n",
      " 19  배아 생성 주요 이유       256351 non-null  int64  \n",
      " 20  총 시술 횟수           256351 non-null  float64\n",
      " 21  클리닉 내 총 시술 횟수     256351 non-null  float64\n",
      " 22  IVF 시술 횟수         256351 non-null  float64\n",
      " 23  DI 시술 횟수          256351 non-null  float64\n",
      " 24  총 임신 횟수           256351 non-null  float64\n",
      " 25  IVF 임신 횟수         256351 non-null  float64\n",
      " 26  DI 임신 횟수          256351 non-null  float64\n",
      " 27  총 출산 횟수           256351 non-null  float64\n",
      " 28  IVF 출산 횟수         256351 non-null  float64\n",
      " 29  DI 출산 횟수          256351 non-null  float64\n",
      " 30  총 생성 배아 수         256351 non-null  float64\n",
      " 31  미세주입된 난자 수        256351 non-null  float64\n",
      " 32  미세주입에서 생성된 배아 수   256351 non-null  float64\n",
      " 33  이식된 배아 수          256351 non-null  float64\n",
      " 34  미세주입 배아 이식 수      256351 non-null  float64\n",
      " 35  저장된 배아 수          256351 non-null  float64\n",
      " 36  미세주입 후 저장된 배아 수   256351 non-null  float64\n",
      " 37  해동된 배아 수          256351 non-null  float64\n",
      " 38  해동 난자 수           256351 non-null  float64\n",
      " 39  수집된 신선 난자 수       256351 non-null  float64\n",
      " 40  저장된 신선 난자 수       256351 non-null  float64\n",
      " 41  혼합된 난자 수          256351 non-null  float64\n",
      " 42  파트너 정자와 혼합된 난자 수  256351 non-null  float64\n",
      " 43  기증자 정자와 혼합된 난자 수  256351 non-null  float64\n",
      " 44  난자 출처             256351 non-null  float64\n",
      " 45  정자 출처             256351 non-null  float64\n",
      " 46  난자 기증자 나이         256351 non-null  float64\n",
      " 47  동결 배아 사용 여부       256351 non-null  float64\n",
      " 48  신선 배아 사용 여부       256351 non-null  float64\n",
      " 49  기증 배아 사용 여부       256351 non-null  float64\n",
      " 50  대리모 여부            256351 non-null  float64\n",
      " 51  총 경과일             256351 non-null  float64\n",
      "dtypes: float64(39), int64(13)\n",
      "memory usage: 101.7 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "배아 생성 주요 이유\n",
       "1    253108\n",
       "0      3243\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_encoded['배아 생성 주요 이유'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 상관 계수 기준으로 제거할 후보 변수들: {'신선 배아 사용 여부', 'IVF 시술 횟수', 'IVF 임신 횟수', 'IVF 출산 횟수', '미세주입에서 생성된 배아 수', '파트너 정자와 혼합된 난자 수'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1785: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.uncentered_tss\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 VIF 기준으로 제거할 후보 변수들: ['시술 유형', '여성 주 불임 원인', '부부 주 불임 원인', '총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수', 'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', '혼합된 난자 수', '파트너 정자와 혼합된 난자 수', '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
      "📌 최종 제거할 변수들: {'신선 배아 사용 여부', 'IVF 출산 횟수', '여성 주 불임 원인', '총 시술 횟수', '시술 유형', 'DI 출산 횟수', '총 생성 배아 수', '혼합된 난자 수', '부부 주 불임 원인', '총 출산 횟수', 'IVF 시술 횟수', '동결 배아 사용 여부', 'IVF 임신 횟수', '미세주입에서 생성된 배아 수', 'DI 시술 횟수', 'DI 임신 횟수', '파트너 정자와 혼합된 난자 수', '미세주입된 난자 수', '총 임신 횟수'}\n",
      "✅ 최종 남은 변수 개수: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# 데이터 정규화 (X_train_encoded & X_test_encoded)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)  # 동일한 스케일 적용\n",
    "\n",
    "# DataFrame 변환 (Feature 이름 유지)\n",
    "feature_names = X_train_encoded.columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "# 상관 행렬 계산\n",
    "correlation_matrix_train = X_train_scaled_df.corr()\n",
    "\n",
    "# 다중 공선성이 높은 칼럼 찾기 (절대 상관 계수가 0.9 이상)\n",
    "threshold = 0.9\n",
    "high_corr_features = set()\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(i + 1, len(feature_names)):\n",
    "        if abs(correlation_matrix_train.iloc[i, j]) > threshold:\n",
    "            high_corr_features.add(feature_names[j])  # 공선성이 높은 컬럼 추가\n",
    "\n",
    "print(f\"📌 상관 계수 기준으로 제거할 후보 변수들: {high_corr_features}\")\n",
    "\n",
    "# 📌 VIF 계산 함수\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# VIF 계산\n",
    "vif_df = calculate_vif(X_train_scaled_df)\n",
    "\n",
    "# VIF 기준(예: 10) 초과하는 변수 제거\n",
    "high_vif_features = vif_df[vif_df[\"VIF\"] > 10][\"Feature\"].tolist()\n",
    "print(f\"📌 VIF 기준으로 제거할 후보 변수들: {high_vif_features}\")\n",
    "\n",
    "# 공선성이 높은 변수 최종 제거 리스트 (상관 계수 + VIF 결합)\n",
    "final_features_to_remove = set(high_corr_features) | set(high_vif_features)\n",
    "print(f\"📌 최종 제거할 변수들: {final_features_to_remove}\")\n",
    "\n",
    "# 다중 공선성이 높은 칼럼 제거\n",
    "X_train_encoded = X_train_scaled_df.drop(columns=final_features_to_remove, errors='ignore')\n",
    "X_test_encoded = X_test_scaled_df.drop(columns=final_features_to_remove, errors='ignore')\n",
    "\n",
    "print(f\"✅ 최종 남은 변수 개수: {X_train_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>시술 시기 코드</th>\n",
       "      <th>시술 당시 나이</th>\n",
       "      <th>특정 시술 유형</th>\n",
       "      <th>배란 자극 여부</th>\n",
       "      <th>단일 배아 이식 여부</th>\n",
       "      <th>착상 전 유전 검사 사용 여부</th>\n",
       "      <th>착상 전 유전 진단 사용 여부</th>\n",
       "      <th>여성 부 불임 원인</th>\n",
       "      <th>부부 부 불임 원인</th>\n",
       "      <th>불명확 불임 원인</th>\n",
       "      <th>...</th>\n",
       "      <th>해동 난자 수</th>\n",
       "      <th>수집된 신선 난자 수</th>\n",
       "      <th>저장된 신선 난자 수</th>\n",
       "      <th>기증자 정자와 혼합된 난자 수</th>\n",
       "      <th>난자 출처</th>\n",
       "      <th>정자 출처</th>\n",
       "      <th>난자 기증자 나이</th>\n",
       "      <th>기증 배아 사용 여부</th>\n",
       "      <th>대리모 여부</th>\n",
       "      <th>총 경과일</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518507</td>\n",
       "      <td>-0.935575</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>-0.578475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-0.184224</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>-1.173581</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.033850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.020975</td>\n",
       "      <td>2.655919</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>-1.836376</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>-0.578475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-1.000848</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>1.803789</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>-1.586105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025910</td>\n",
       "      <td>-0.935575</td>\n",
       "      <td>0.672585</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>-0.578475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-0.048121</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>-1.173581</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>-0.506135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.471622</td>\n",
       "      <td>-0.217277</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>-0.578475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-0.456432</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.480513</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>-1.586105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025910</td>\n",
       "      <td>-0.935575</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>-0.578475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-0.184224</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>-1.173581</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.033850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256346</th>\n",
       "      <td>1.020975</td>\n",
       "      <td>-0.935575</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>1.728684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>0.360191</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>-1.173581</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>1.113821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256347</th>\n",
       "      <td>1.020975</td>\n",
       "      <td>0.501022</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>-0.578475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-0.456432</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.811332</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.033850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256348</th>\n",
       "      <td>0.025910</td>\n",
       "      <td>-0.217277</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>-0.578475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>0.224087</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.480513</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.033850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256349</th>\n",
       "      <td>1.518507</td>\n",
       "      <td>0.501022</td>\n",
       "      <td>-0.962962</td>\n",
       "      <td>0.544551</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>1.728684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-0.728640</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.811332</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>-1.046120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256350</th>\n",
       "      <td>0.523442</td>\n",
       "      <td>-0.217277</td>\n",
       "      <td>0.672585</td>\n",
       "      <td>-1.836376</td>\n",
       "      <td>-0.543057</td>\n",
       "      <td>-0.103519</td>\n",
       "      <td>-0.11236</td>\n",
       "      <td>-0.112199</td>\n",
       "      <td>-0.094036</td>\n",
       "      <td>1.728684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-1.136951</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.220261</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.480513</td>\n",
       "      <td>-0.098393</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>-1.046120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256351 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        시술 시기 코드  시술 당시 나이  특정 시술 유형  배란 자극 여부  단일 배아 이식 여부  착상 전 유전 검사 사용 여부  \\\n",
       "0       1.518507 -0.935575 -0.962962  0.544551    -0.543057         -0.103519   \n",
       "1       1.020975  2.655919 -0.962962 -1.836376    -0.543057         -0.103519   \n",
       "2       0.025910 -0.935575  0.672585  0.544551    -0.543057         -0.103519   \n",
       "3      -0.471622 -0.217277 -0.962962  0.544551    -0.543057         -0.103519   \n",
       "4       0.025910 -0.935575 -0.962962  0.544551    -0.543057         -0.103519   \n",
       "...          ...       ...       ...       ...          ...               ...   \n",
       "256346  1.020975 -0.935575 -0.962962  0.544551    -0.543057         -0.103519   \n",
       "256347  1.020975  0.501022 -0.962962  0.544551    -0.543057         -0.103519   \n",
       "256348  0.025910 -0.217277 -0.962962  0.544551    -0.543057         -0.103519   \n",
       "256349  1.518507  0.501022 -0.962962  0.544551    -0.543057         -0.103519   \n",
       "256350  0.523442 -0.217277  0.672585 -1.836376    -0.543057         -0.103519   \n",
       "\n",
       "        착상 전 유전 진단 사용 여부  여성 부 불임 원인  부부 부 불임 원인  불명확 불임 원인  ...   해동 난자 수  \\\n",
       "0               -0.11236   -0.112199   -0.094036  -0.578475  ... -0.066583   \n",
       "1               -0.11236   -0.112199   -0.094036  -0.578475  ... -0.066583   \n",
       "2               -0.11236   -0.112199   -0.094036  -0.578475  ... -0.066583   \n",
       "3               -0.11236   -0.112199   -0.094036  -0.578475  ... -0.066583   \n",
       "4               -0.11236   -0.112199   -0.094036  -0.578475  ... -0.066583   \n",
       "...                  ...         ...         ...        ...  ...       ...   \n",
       "256346          -0.11236   -0.112199   -0.094036   1.728684  ... -0.066583   \n",
       "256347          -0.11236   -0.112199   -0.094036  -0.578475  ... -0.066583   \n",
       "256348          -0.11236   -0.112199   -0.094036  -0.578475  ... -0.066583   \n",
       "256349          -0.11236   -0.112199   -0.094036   1.728684  ... -0.066583   \n",
       "256350          -0.11236   -0.112199   -0.094036   1.728684  ... -0.066583   \n",
       "\n",
       "        수집된 신선 난자 수  저장된 신선 난자 수  기증자 정자와 혼합된 난자 수     난자 출처     정자 출처  \\\n",
       "0         -0.184224    -0.083423         -0.220261  0.256018  0.344047   \n",
       "1         -1.000848    -0.083423         -0.220261  0.256018  0.344047   \n",
       "2         -0.048121    -0.083423         -0.220261  0.256018  0.344047   \n",
       "3         -0.456432    -0.083423         -0.220261  0.256018  0.344047   \n",
       "4         -0.184224    -0.083423         -0.220261  0.256018  0.344047   \n",
       "...             ...          ...               ...       ...       ...   \n",
       "256346     0.360191    -0.083423         -0.220261  0.256018  0.344047   \n",
       "256347    -0.456432    -0.083423         -0.220261  0.256018  0.344047   \n",
       "256348     0.224087    -0.083423         -0.220261  0.256018  0.344047   \n",
       "256349    -0.728640    -0.083423         -0.220261  0.256018  0.344047   \n",
       "256350    -1.136951    -0.083423         -0.220261  0.256018  0.344047   \n",
       "\n",
       "        난자 기증자 나이  기증 배아 사용 여부  대리모 여부     총 경과일  \n",
       "0       -1.173581    -0.098393 -0.0641  0.033850  \n",
       "1        1.803789    -0.098393 -0.0641 -1.586105  \n",
       "2       -1.173581    -0.098393 -0.0641 -0.506135  \n",
       "3        0.480513    -0.098393 -0.0641 -1.586105  \n",
       "4       -1.173581    -0.098393 -0.0641  0.033850  \n",
       "...           ...          ...     ...       ...  \n",
       "256346  -1.173581    -0.098393 -0.0641  1.113821  \n",
       "256347   0.811332    -0.098393 -0.0641  0.033850  \n",
       "256348   0.480513    -0.098393 -0.0641  0.033850  \n",
       "256349   0.811332    -0.098393 -0.0641 -1.046120  \n",
       "256350   0.480513    -0.098393 -0.0641 -1.046120  \n",
       "\n",
       "[256351 rows x 33 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;ExtraTreesClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\">?<span>Documentation for ExtraTreesClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>ExtraTreesClassifier(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesClassifier(random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train_encoded, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**앙상블 및 Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5)\n",
    "    }\n",
    "    model = XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    scores = cross_val_score(model, X_train_encoded, y, cv=5, scoring=\"roc_auc\")\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lightgbm(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "    }\n",
    "    model = LGBMClassifier(**params, random_state=42)\n",
    "    scores = cross_val_score(model, X_train_encoded, y, cv=5, scoring=\"roc_auc\")  \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 3, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4)\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    scores = cross_val_score(model, X_train_encoded, y, cv=5, scoring=\"roc_auc\")  \n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:33:34,099] A new study created in memory with name: no-name-51db5fbd-5320-4d0f-8fac-c4f9b41e9f43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:33:38,498] Trial 0 finished with value: 0.7340837002314029 and parameters: {'n_estimators': 283, 'max_depth': 9, 'learning_rate': 0.18680693288094835, 'subsample': 0.6303907352649968, 'colsample_bytree': 0.9035942048740728, 'gamma': 3.8301109691770643}. Best is trial 0 with value: 0.7340837002314029.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:33:42,501] Trial 1 finished with value: 0.7340578774487525 and parameters: {'n_estimators': 291, 'max_depth': 8, 'learning_rate': 0.26062514635325235, 'subsample': 0.944657107632822, 'colsample_bytree': 0.6645080854111852, 'gamma': 2.679183554922747}. Best is trial 0 with value: 0.7340837002314029.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:33:46,150] Trial 2 finished with value: 0.7351194247877277 and parameters: {'n_estimators': 254, 'max_depth': 4, 'learning_rate': 0.2654022363583577, 'subsample': 0.9426592084043928, 'colsample_bytree': 0.9715840170716382, 'gamma': 1.6173328470378907}. Best is trial 2 with value: 0.7351194247877277.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:33:50,301] Trial 3 finished with value: 0.735014381468386 and parameters: {'n_estimators': 309, 'max_depth': 4, 'learning_rate': 0.23997487295525624, 'subsample': 0.8251491451434252, 'colsample_bytree': 0.772186277804219, 'gamma': 4.599354787455231}. Best is trial 2 with value: 0.7351194247877277.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:33:54,219] Trial 4 finished with value: 0.7344806790085964 and parameters: {'n_estimators': 282, 'max_depth': 9, 'learning_rate': 0.24143459746273108, 'subsample': 0.7764024978005886, 'colsample_bytree': 0.7529366575109949, 'gamma': 4.178004686638419}. Best is trial 2 with value: 0.7351194247877277.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:33:56,515] Trial 5 finished with value: 0.7344947641631455 and parameters: {'n_estimators': 133, 'max_depth': 7, 'learning_rate': 0.2883790414580966, 'subsample': 0.7936452461195144, 'colsample_bytree': 0.6619519888255193, 'gamma': 2.824369641603417}. Best is trial 2 with value: 0.7351194247877277.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:33:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:01,664] Trial 6 finished with value: 0.7301856759718437 and parameters: {'n_estimators': 348, 'max_depth': 10, 'learning_rate': 0.2536493021469368, 'subsample': 0.984713524589291, 'colsample_bytree': 0.9988063361317001, 'gamma': 0.8516856474209084}. Best is trial 2 with value: 0.7351194247877277.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:04,082] Trial 7 finished with value: 0.7327873653217564 and parameters: {'n_estimators': 131, 'max_depth': 10, 'learning_rate': 0.280698363477873, 'subsample': 0.6766155724454106, 'colsample_bytree': 0.7609211125242131, 'gamma': 3.5439674172858404}. Best is trial 2 with value: 0.7351194247877277.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:09,020] Trial 8 finished with value: 0.7354990918285523 and parameters: {'n_estimators': 236, 'max_depth': 9, 'learning_rate': 0.03379089668804304, 'subsample': 0.8169797109718816, 'colsample_bytree': 0.6280648448707399, 'gamma': 4.417348316814003}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:13,584] Trial 9 finished with value: 0.7352349898959822 and parameters: {'n_estimators': 240, 'max_depth': 3, 'learning_rate': 0.1099776196432535, 'subsample': 0.7218512106915145, 'colsample_bytree': 0.964650067100501, 'gamma': 3.0729536327048663}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:25,099] Trial 10 finished with value: 0.7354815278505974 and parameters: {'n_estimators': 498, 'max_depth': 12, 'learning_rate': 0.012698397414207158, 'subsample': 0.8740925096784187, 'colsample_bytree': 0.6036638033746263, 'gamma': 4.956241674772715}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:34,724] Trial 11 finished with value: 0.7354589965042633 and parameters: {'n_estimators': 473, 'max_depth': 12, 'learning_rate': 0.01605817835261548, 'subsample': 0.8715217892334002, 'colsample_bytree': 0.6003204684972515, 'gamma': 4.984354580755203}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:45,440] Trial 12 finished with value: 0.7354466522479246 and parameters: {'n_estimators': 491, 'max_depth': 12, 'learning_rate': 0.014499354041944586, 'subsample': 0.872048897345391, 'colsample_bytree': 0.6012376670952438, 'gamma': 4.997762685273633}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:51,940] Trial 13 finished with value: 0.7353755772323506 and parameters: {'n_estimators': 399, 'max_depth': 6, 'learning_rate': 0.0706591952287086, 'subsample': 0.8638381740849318, 'colsample_bytree': 0.6850333628480443, 'gamma': 1.859115649801221}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:34:55,803] Trial 14 finished with value: 0.7352619685633869 and parameters: {'n_estimators': 185, 'max_depth': 11, 'learning_rate': 0.05977646794513614, 'subsample': 0.7549144477037641, 'colsample_bytree': 0.878235577378042, 'gamma': 4.269554411155841}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:34:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:03,653] Trial 15 finished with value: 0.730872316561335 and parameters: {'n_estimators': 393, 'max_depth': 6, 'learning_rate': 0.1333429839374594, 'subsample': 0.8312534356389074, 'colsample_bytree': 0.7132100368443518, 'gamma': 0.09446246234695987}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:07,341] Trial 16 finished with value: 0.7345771322811899 and parameters: {'n_estimators': 185, 'max_depth': 11, 'learning_rate': 0.06705522736105718, 'subsample': 0.9140127451064706, 'colsample_bytree': 0.8373466639987925, 'gamma': 3.508156542232183}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:16,180] Trial 17 finished with value: 0.7350077413904019 and parameters: {'n_estimators': 445, 'max_depth': 8, 'learning_rate': 0.03941626647610381, 'subsample': 0.7509507012865111, 'colsample_bytree': 0.6355387978352328, 'gamma': 1.9373509930076112}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:19,755] Trial 18 finished with value: 0.7352834353561424 and parameters: {'n_estimators': 196, 'max_depth': 10, 'learning_rate': 0.09366994705392552, 'subsample': 0.7059273323423683, 'colsample_bytree': 0.7141041544307486, 'gamma': 4.4104852621063575}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:24,797] Trial 19 finished with value: 0.7345498898792023 and parameters: {'n_estimators': 358, 'max_depth': 12, 'learning_rate': 0.1409448282699865, 'subsample': 0.9019113928318017, 'colsample_bytree': 0.8125482888870392, 'gamma': 3.8000630331469623}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:30,663] Trial 20 finished with value: 0.7348180310372483 and parameters: {'n_estimators': 434, 'max_depth': 7, 'learning_rate': 0.1963629885407558, 'subsample': 0.8264827692145512, 'colsample_bytree': 0.6284968742837604, 'gamma': 3.259437702057065}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:40,794] Trial 21 finished with value: 0.7354261132651159 and parameters: {'n_estimators': 490, 'max_depth': 12, 'learning_rate': 0.013789528455777448, 'subsample': 0.8656916354829054, 'colsample_bytree': 0.6034490833166621, 'gamma': 4.984663761103251}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:47,915] Trial 22 finished with value: 0.7352698038886867 and parameters: {'n_estimators': 459, 'max_depth': 11, 'learning_rate': 0.04174581087189061, 'subsample': 0.999158509340846, 'colsample_bytree': 0.6437977856092635, 'gamma': 4.619206840898674}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:35:58,958] Trial 23 finished with value: 0.7353957068850729 and parameters: {'n_estimators': 498, 'max_depth': 12, 'learning_rate': 0.010997082906324996, 'subsample': 0.8977270493842069, 'colsample_bytree': 0.701869219356709, 'gamma': 4.934986414573447}. Best is trial 8 with value: 0.7354990918285523.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:35:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:36:05,957] Trial 24 finished with value: 0.7355761302088263 and parameters: {'n_estimators': 410, 'max_depth': 9, 'learning_rate': 0.04429291094984883, 'subsample': 0.8432019183270041, 'colsample_bytree': 0.6015480730177228, 'gamma': 3.8713817890813846}. Best is trial 24 with value: 0.7355761302088263.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:36:11,715] Trial 25 finished with value: 0.7351621570188507 and parameters: {'n_estimators': 405, 'max_depth': 9, 'learning_rate': 0.10120211053165486, 'subsample': 0.8110548866815351, 'colsample_bytree': 0.6702986644600506, 'gamma': 4.089254356726615}. Best is trial 24 with value: 0.7355761302088263.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:36:17,728] Trial 26 finished with value: 0.7355622784009518 and parameters: {'n_estimators': 339, 'max_depth': 6, 'learning_rate': 0.0506254131548686, 'subsample': 0.7842078127309913, 'colsample_bytree': 0.6358617435001691, 'gamma': 2.301338237391517}. Best is trial 24 with value: 0.7355761302088263.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:36:23,684] Trial 27 finished with value: 0.7355470397388074 and parameters: {'n_estimators': 324, 'max_depth': 6, 'learning_rate': 0.042558493928633395, 'subsample': 0.7730345097050546, 'colsample_bytree': 0.7258640867652209, 'gamma': 2.214089350261569}. Best is trial 24 with value: 0.7355761302088263.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:36:29,579] Trial 28 finished with value: 0.734561092835566 and parameters: {'n_estimators': 331, 'max_depth': 6, 'learning_rate': 0.08217568988239934, 'subsample': 0.7755409294196208, 'colsample_bytree': 0.7374541378730054, 'gamma': 1.3673958308965972}. Best is trial 24 with value: 0.7355761302088263.\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:36:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-02-16 22:36:35,485] Trial 29 finished with value: 0.7336116351792509 and parameters: {'n_estimators': 376, 'max_depth': 5, 'learning_rate': 0.1925767220203342, 'subsample': 0.6075226463642645, 'colsample_bytree': 0.689124048471411, 'gamma': 2.269973110924495}. Best is trial 24 with value: 0.7355761302088263.\n",
      "[I 2025-02-16 22:36:35,485] A new study created in memory with name: no-name-cce00b0d-db35-475d-959a-c7a4394c8cd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:36:39,492] Trial 0 finished with value: 0.7343432320967155 and parameters: {'n_estimators': 190, 'max_depth': 8, 'learning_rate': 0.01812039018286032, 'num_leaves': 30, 'subsample': 0.7636237486535218}. Best is trial 0 with value: 0.7343432320967155.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:36:42,787] Trial 1 finished with value: 0.7329508066382451 and parameters: {'n_estimators': 271, 'max_depth': 8, 'learning_rate': 0.21718632920672357, 'num_leaves': 19, 'subsample': 0.7331793966365048}. Best is trial 0 with value: 0.7343432320967155.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:36:45,682] Trial 2 finished with value: 0.731890411499142 and parameters: {'n_estimators': 159, 'max_depth': 11, 'learning_rate': 0.18759080156484048, 'num_leaves': 44, 'subsample': 0.6938730619934}. Best is trial 0 with value: 0.7343432320967155.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:36:49,370] Trial 3 finished with value: 0.7350620508593166 and parameters: {'n_estimators': 312, 'max_depth': 11, 'learning_rate': 0.1074220097992988, 'num_leaves': 12, 'subsample': 0.9382481999292298}. Best is trial 3 with value: 0.7350620508593166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:36:51,942] Trial 4 finished with value: 0.7351313330232794 and parameters: {'n_estimators': 160, 'max_depth': 11, 'learning_rate': 0.09228066944697007, 'num_leaves': 10, 'subsample': 0.6279974862148351}. Best is trial 4 with value: 0.7351313330232794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:36:54,986] Trial 5 finished with value: 0.7344267589025677 and parameters: {'n_estimators': 244, 'max_depth': 9, 'learning_rate': 0.1953758905302927, 'num_leaves': 11, 'subsample': 0.883378862691973}. Best is trial 4 with value: 0.7351313330232794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:01,259] Trial 6 finished with value: 0.7331484886120929 and parameters: {'n_estimators': 495, 'max_depth': 12, 'learning_rate': 0.07629602624537651, 'num_leaves': 32, 'subsample': 0.9320515182104463}. Best is trial 4 with value: 0.7351313330232794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:05,099] Trial 7 finished with value: 0.7349920667373413 and parameters: {'n_estimators': 224, 'max_depth': 7, 'learning_rate': 0.06672817982295463, 'num_leaves': 36, 'subsample': 0.8732826039038892}. Best is trial 4 with value: 0.7351313330232794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:09,163] Trial 8 finished with value: 0.7334663001865509 and parameters: {'n_estimators': 352, 'max_depth': 12, 'learning_rate': 0.13786123412800774, 'num_leaves': 19, 'subsample': 0.8034980446646672}. Best is trial 4 with value: 0.7351313330232794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:12,262] Trial 9 finished with value: 0.7340039271449287 and parameters: {'n_estimators': 183, 'max_depth': 12, 'learning_rate': 0.1268975723541606, 'num_leaves': 33, 'subsample': 0.7215528607319508}. Best is trial 4 with value: 0.7351313330232794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:14,301] Trial 10 finished with value: 0.7350280635192152 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.2800450217825473, 'num_leaves': 50, 'subsample': 0.6230854832944968}. Best is trial 4 with value: 0.7351313330232794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:18,579] Trial 11 finished with value: 0.735224937128958 and parameters: {'n_estimators': 385, 'max_depth': 10, 'learning_rate': 0.08600689016807848, 'num_leaves': 10, 'subsample': 0.9824121378161383}. Best is trial 11 with value: 0.735224937128958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:24,313] Trial 12 finished with value: 0.735221751574875 and parameters: {'n_estimators': 402, 'max_depth': 10, 'learning_rate': 0.023101886994935233, 'num_leaves': 19, 'subsample': 0.607793295019505}. Best is trial 11 with value: 0.735224937128958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:30,797] Trial 13 finished with value: 0.7345167454206603 and parameters: {'n_estimators': 417, 'max_depth': 6, 'learning_rate': 0.011774978993663823, 'num_leaves': 20, 'subsample': 0.8262923566220683}. Best is trial 11 with value: 0.735224937128958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:36,686] Trial 14 finished with value: 0.7351113331519772 and parameters: {'n_estimators': 409, 'max_depth': 9, 'learning_rate': 0.042046206873135726, 'num_leaves': 25, 'subsample': 0.974253169339839}. Best is trial 11 with value: 0.735224937128958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:41,937] Trial 15 finished with value: 0.735325858698204 and parameters: {'n_estimators': 399, 'max_depth': 10, 'learning_rate': 0.05102181402132013, 'num_leaves': 16, 'subsample': 0.9988320587921031}. Best is trial 15 with value: 0.735325858698204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:47,659] Trial 16 finished with value: 0.7351535794073067 and parameters: {'n_estimators': 487, 'max_depth': 5, 'learning_rate': 0.054637723835967295, 'num_leaves': 15, 'subsample': 0.989605753210734}. Best is trial 15 with value: 0.735325858698204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:52,062] Trial 17 finished with value: 0.7316205142233155 and parameters: {'n_estimators': 366, 'max_depth': 10, 'learning_rate': 0.16864649521179206, 'num_leaves': 25, 'subsample': 0.9235659316219693}. Best is trial 15 with value: 0.735325858698204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:37:56,916] Trial 18 finished with value: 0.7341885073990331 and parameters: {'n_estimators': 454, 'max_depth': 9, 'learning_rate': 0.10983725378910483, 'num_leaves': 15, 'subsample': 0.8775714092939717}. Best is trial 15 with value: 0.735325858698204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:00,772] Trial 19 finished with value: 0.7305458767014441 and parameters: {'n_estimators': 322, 'max_depth': 10, 'learning_rate': 0.2445658618393205, 'num_leaves': 24, 'subsample': 0.9997344998475458}. Best is trial 15 with value: 0.735325858698204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:05,921] Trial 20 finished with value: 0.7303992600659389 and parameters: {'n_estimators': 367, 'max_depth': 7, 'learning_rate': 0.1543080945599984, 'num_leaves': 40, 'subsample': 0.9582965732361446}. Best is trial 15 with value: 0.735325858698204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:10,954] Trial 21 finished with value: 0.7353422844407314 and parameters: {'n_estimators': 406, 'max_depth': 10, 'learning_rate': 0.041225232291948556, 'num_leaves': 15, 'subsample': 0.6637715577700649}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:16,245] Trial 22 finished with value: 0.735255343865514 and parameters: {'n_estimators': 445, 'max_depth': 10, 'learning_rate': 0.04740865383183875, 'num_leaves': 15, 'subsample': 0.6859622475920141}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:21,636] Trial 23 finished with value: 0.7353176113154212 and parameters: {'n_estimators': 448, 'max_depth': 9, 'learning_rate': 0.0436691333027741, 'num_leaves': 15, 'subsample': 0.680598714050596}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:27,464] Trial 24 finished with value: 0.735154398201708 and parameters: {'n_estimators': 445, 'max_depth': 9, 'learning_rate': 0.03992425724044032, 'num_leaves': 22, 'subsample': 0.6624275572828876}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:32,674] Trial 25 finished with value: 0.7347956041549917 and parameters: {'n_estimators': 466, 'max_depth': 8, 'learning_rate': 0.06965068293377016, 'num_leaves': 15, 'subsample': 0.6587964491970317}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:37,932] Trial 26 finished with value: 0.7352022583851248 and parameters: {'n_estimators': 338, 'max_depth': 11, 'learning_rate': 0.031940931434624606, 'num_leaves': 29, 'subsample': 0.7630037151165942}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:42,805] Trial 27 finished with value: 0.734218409776344 and parameters: {'n_estimators': 425, 'max_depth': 6, 'learning_rate': 0.1038848343547752, 'num_leaves': 17, 'subsample': 0.7454792129850953}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:47,035] Trial 28 finished with value: 0.7350837119151954 and parameters: {'n_estimators': 273, 'max_depth': 9, 'learning_rate': 0.05301922087425711, 'num_leaves': 27, 'subsample': 0.8363539971843583}. Best is trial 21 with value: 0.7353422844407314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n",
      "[LightGBM] [Info] Start training from score -1.054554\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n",
      "[LightGBM] [Info] Start training from score -1.054579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:38:52,388] Trial 29 finished with value: 0.7344942356726714 and parameters: {'n_estimators': 385, 'max_depth': 8, 'learning_rate': 0.016194661889882528, 'num_leaves': 13, 'subsample': 0.7808343924978168}. Best is trial 21 with value: 0.7353422844407314.\n",
      "[I 2025-02-16 22:38:52,389] A new study created in memory with name: no-name-f7429d9a-c2db-4f28-899d-9f7fa9432ff4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 22:40:35,758] Trial 0 finished with value: 0.7327288979777066 and parameters: {'n_estimators': 241, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.7327288979777066.\n",
      "[I 2025-02-16 22:41:14,395] Trial 1 finished with value: 0.7248345783744777 and parameters: {'n_estimators': 156, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.7327288979777066.\n",
      "[I 2025-02-16 22:41:48,491] Trial 2 finished with value: 0.722071890858957 and parameters: {'n_estimators': 161, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.7327288979777066.\n",
      "[I 2025-02-16 22:42:34,789] Trial 3 finished with value: 0.7287759426396456 and parameters: {'n_estimators': 146, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.7327288979777066.\n",
      "[I 2025-02-16 22:43:40,325] Trial 4 finished with value: 0.733252572571027 and parameters: {'n_estimators': 145, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.733252572571027.\n",
      "[I 2025-02-16 22:44:38,245] Trial 5 finished with value: 0.7321158608248941 and parameters: {'n_estimators': 144, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 4 with value: 0.733252572571027.\n",
      "[I 2025-02-16 22:45:13,778] Trial 6 finished with value: 0.7221552875441235 and parameters: {'n_estimators': 169, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.733252572571027.\n",
      "[I 2025-02-16 22:45:51,816] Trial 7 finished with value: 0.7300298315910776 and parameters: {'n_estimators': 111, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 4 with value: 0.733252572571027.\n",
      "[I 2025-02-16 22:47:09,912] Trial 8 finished with value: 0.7321353253662062 and parameters: {'n_estimators': 195, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.733252572571027.\n",
      "[I 2025-02-16 22:47:41,544] Trial 9 finished with value: 0.7286769718123896 and parameters: {'n_estimators': 101, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.733252572571027.\n",
      "[I 2025-02-16 22:49:44,204] Trial 10 finished with value: 0.7332703653503752 and parameters: {'n_estimators': 273, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.7332703653503752.\n",
      "[I 2025-02-16 22:51:57,839] Trial 11 finished with value: 0.7332709748623687 and parameters: {'n_estimators': 297, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.7332709748623687.\n",
      "[I 2025-02-16 22:54:12,077] Trial 12 finished with value: 0.7333648486707276 and parameters: {'n_estimators': 299, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 22:56:01,446] Trial 13 finished with value: 0.7312327029689665 and parameters: {'n_estimators': 294, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 22:57:59,351] Trial 14 finished with value: 0.7333466372719867 and parameters: {'n_estimators': 262, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 22:59:37,711] Trial 15 finished with value: 0.7321619577580085 and parameters: {'n_estimators': 247, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:01:20,320] Trial 16 finished with value: 0.7328667027931153 and parameters: {'n_estimators': 240, 'max_depth': 11, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:02:33,535] Trial 17 finished with value: 0.7268258403601913 and parameters: {'n_estimators': 262, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:03:52,806] Trial 18 finished with value: 0.73123289695338 and parameters: {'n_estimators': 213, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:05:48,424] Trial 19 finished with value: 0.7329155785448422 and parameters: {'n_estimators': 272, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:06:26,698] Trial 20 finished with value: 0.7198568997375434 and parameters: {'n_estimators': 219, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:08:41,425] Trial 21 finished with value: 0.7332724102427782 and parameters: {'n_estimators': 300, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:10:48,997] Trial 22 finished with value: 0.7332905411852901 and parameters: {'n_estimators': 284, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:12:48,246] Trial 23 finished with value: 0.7329020998759989 and parameters: {'n_estimators': 281, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:14:22,913] Trial 24 finished with value: 0.731217053075681 and parameters: {'n_estimators': 254, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 12 with value: 0.7333648486707276.\n",
      "[I 2025-02-16 23:16:27,347] Trial 25 finished with value: 0.7334696133376359 and parameters: {'n_estimators': 278, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7334696133376359.\n",
      "[I 2025-02-16 23:17:59,481] Trial 26 finished with value: 0.7321768871055607 and parameters: {'n_estimators': 231, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7334696133376359.\n",
      "[I 2025-02-16 23:19:51,258] Trial 27 finished with value: 0.7328675237066327 and parameters: {'n_estimators': 263, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7334696133376359.\n",
      "[I 2025-02-16 23:21:17,280] Trial 28 finished with value: 0.7332167587973633 and parameters: {'n_estimators': 192, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7334696133376359.\n",
      "[I 2025-02-16 23:22:37,382] Trial 29 finished with value: 0.73015708840658 and parameters: {'n_estimators': 234, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.7334696133376359.\n"
     ]
    }
   ],
   "source": [
    "# Optuna 스터디 실행\n",
    "print(\"Optimizing XGBoost...\")\n",
    "xgb_study = optuna.create_study(direction=\"maximize\")\n",
    "xgb_study.optimize(optimize_xgboost, n_trials=30)\n",
    "best_xgb_params = xgb_study.best_params\n",
    "\n",
    "print(\"Optimizing LightGBM...\")\n",
    "lgbm_study = optuna.create_study(direction=\"maximize\")\n",
    "lgbm_study.optimize(optimize_lightgbm, n_trials=30)\n",
    "best_lgbm_params = lgbm_study.best_params\n",
    "\n",
    "print(\"Optimizing RandomForest...\")\n",
    "rf_study = optuna.create_study(direction=\"maximize\")\n",
    "rf_study.optimize(optimize_random_forest, n_trials=30)\n",
    "best_rf_params = rf_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화된 모델 생성\n",
    "xgb_model = XGBClassifier(**best_xgb_params, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "lgbm_model = LGBMClassifier(**best_lgbm_params, random_state=42)\n",
    "rf_model = RandomForestClassifier(**best_rf_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Best Parameters:\n",
      "{'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6015480730177228, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'gamma': 3.8713817890813846, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.04429291094984883, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 410, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8432019183270041, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n",
      "\n",
      "LightGBM Best Parameters:\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.041225232291948556, 'max_depth': 10, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 406, 'n_jobs': None, 'num_leaves': 15, 'objective': None, 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.6637715577700649, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "RandomForest Best Parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 12, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 4, 'min_samples_split': 9, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 278, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 모델의 파라미터 확인\n",
    "print(\"XGBoost Best Parameters:\")\n",
    "print(xgb_model.get_params())\n",
    "\n",
    "# LightGBM 모델의 파라미터 확인\n",
    "print(\"\\nLightGBM Best Parameters:\")\n",
    "print(lgbm_model.get_params())\n",
    "\n",
    "# RandomForest 모델의 파라미터 확인\n",
    "print(\"\\nRandomForest Best Parameters:\")\n",
    "print(rf_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting 앙상블\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"xgb\", xgb_model),\n",
    "        (\"lgbm\", lgbm_model),\n",
    "        (\"rf\", rf_model)\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoyeong\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:22:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 66228, number of negative: 190123\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 387\n",
      "[LightGBM] [Info] Number of data points in the train set: 256351, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258349 -> initscore=-1.054568\n",
      "[LightGBM] [Info] Start training from score -1.054568\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.6015480730177228,\n",
       "                                            device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=&#x27;logloss&#x27;,\n",
       "                                            feature_types=None,\n",
       "                                            gamma=3.8713817890813846,\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interac...\n",
       "                                            n_estimators=410, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=42, ...)),\n",
       "                             (&#x27;lgbm&#x27;,\n",
       "                              LGBMClassifier(learning_rate=0.041225232291948556,\n",
       "                                             max_depth=10, n_estimators=406,\n",
       "                                             num_leaves=15, random_state=42,\n",
       "                                             subsample=0.6637715577700649)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(max_depth=12,\n",
       "                                                     min_samples_leaf=4,\n",
       "                                                     min_samples_split=9,\n",
       "                                                     n_estimators=278,\n",
       "                                                     random_state=42))],\n",
       "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;VotingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.VotingClassifier.html\">?<span>Documentation for VotingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>VotingClassifier(estimators=[(&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.6015480730177228,\n",
       "                                            device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=&#x27;logloss&#x27;,\n",
       "                                            feature_types=None,\n",
       "                                            gamma=3.8713817890813846,\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interac...\n",
       "                                            n_estimators=410, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=42, ...)),\n",
       "                             (&#x27;lgbm&#x27;,\n",
       "                              LGBMClassifier(learning_rate=0.041225232291948556,\n",
       "                                             max_depth=10, n_estimators=406,\n",
       "                                             num_leaves=15, random_state=42,\n",
       "                                             subsample=0.6637715577700649)),\n",
       "                             (&#x27;rf&#x27;,\n",
       "                              RandomForestClassifier(max_depth=12,\n",
       "                                                     min_samples_leaf=4,\n",
       "                                                     min_samples_split=9,\n",
       "                                                     n_estimators=278,\n",
       "                                                     random_state=42))],\n",
       "                 voting=&#x27;soft&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.6015480730177228, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;logloss&#x27;, feature_types=None,\n",
       "              gamma=3.8713817890813846, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.04429291094984883,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=410, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>lgbm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(learning_rate=0.041225232291948556, max_depth=10,\n",
       "               n_estimators=406, num_leaves=15, random_state=42,\n",
       "               subsample=0.6637715577700649)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=12, min_samples_leaf=4, min_samples_split=9,\n",
       "                       n_estimators=278, random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.6015480730177228,\n",
       "                                            device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric='logloss',\n",
       "                                            feature_types=None,\n",
       "                                            gamma=3.8713817890813846,\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interac...\n",
       "                                            n_estimators=410, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=42, ...)),\n",
       "                             ('lgbm',\n",
       "                              LGBMClassifier(learning_rate=0.041225232291948556,\n",
       "                                             max_depth=10, n_estimators=406,\n",
       "                                             num_leaves=15, random_state=42,\n",
       "                                             subsample=0.6637715577700649)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(max_depth=12,\n",
       "                                                     min_samples_leaf=4,\n",
       "                                                     min_samples_split=9,\n",
       "                                                     n_estimators=278,\n",
       "                                                     random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 데이터로 학습\n",
    "ensemble_model.fit(X_train_encoded, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = ensemble_model.predict_proba(X_test_encoded)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../../sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('./Result/baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00419399, 0.00949558, 0.14818891, ..., 0.46271164, 0.17253709,\n",
       "       0.00435354])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.15686441e-04 3.18624443e-04 3.20559992e-04 ... 6.37033961e-01\n",
      " 6.38688690e-01 6.55065360e-01] [1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(sample_submission['probability'], return_counts=True)\n",
    "print(unique, counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
